.TH "find_worker_executing_work" 9 "find_worker_executing_work" "May 2021" "Kernel Hacker's Manual" LINUX
.SH NAME
find_worker_executing_work \- find worker which is executing a work
.SH SYNOPSIS
.B "struct worker *" find_worker_executing_work
.BI "(struct worker_pool *pool "  ","
.BI "struct work_struct *work "  ");"
.SH ARGUMENTS
.IP "pool" 12
pool of interest
.IP "work" 12
work to find worker for
.SH "DESCRIPTION"
Find a worker which is executing \fIwork\fP on \fIpool\fP by searching
\fIpool->busy_hash\fP which is keyed by the address of \fIwork\fP.  For a worker
to match, its current execution should match the address of \fIwork\fP and
its work function.  This is to avoid unwanted dependency between
unrelated work executions through a work item being recycled while still
being executed.

This is a bit tricky.  A work item may be freed once its execution
starts and nothing prevents the freed area from being recycled for
another work item.  If the same work item address ends up being reused
before the original execution finishes, workqueue will identify the
recycled work item as currently executing and make it wait until the
current execution finishes, introducing an unwanted dependency.

This function checks the work item address and work function to avoid
false positives.  Note that this isn't complete as one may construct a
work function which can introduce dependency onto itself through a
recycled work item.  Well, if somebody wants to shoot oneself in the
foot that badly, there's only so much we can do, and if such deadlock
actually occurs, it should be easy to locate the culprit work function.
.SH "CONTEXT"
raw_spin_lock_irq(pool->lock).
.SH "RETURN"
Pointer to worker which is executing \fIwork\fP if found, NULL
otherwise.
