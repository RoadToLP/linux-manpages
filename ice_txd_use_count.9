.TH "ice_txd_use_count" 9 "ice_txd_use_count" "May 2021" "Kernel Hacker's Manual" LINUX
.SH NAME
ice_txd_use_count \- estimate the number of descriptors needed for Tx
.SH SYNOPSIS
.B "unsigned int" ice_txd_use_count
.BI "(unsigned int size "  ");"
.SH ARGUMENTS
.IP "size" 12
transmit request size in bytes
.SH "DESCRIPTION"
Due to hardware alignment restrictions (4K alignment), we need to
assume that we can have no more than 12K of data per descriptor, even
though each descriptor can take up to 16K - 1 bytes of aligned memory.
Thus, we need to divide by 12K. But division is slow! Instead,
we decompose the operation into shifts and one relatively cheap
multiply operation.

To divide by 12K, we first divide by 4K, then divide by 3:
To divide by 4K, shift right by 12 bits
To divide by 3, multiply by 85, then divide by 256
(Divide by 256 is done by shifting right by 8 bits)
Finally, we add one to round up. Because 256 isn't an exact multiple of
3, we'll underestimate near each multiple of 12K. This is actually more
accurate as we have 4K - 1 of wiggle room that we can fit into the last
segment. For our purposes this is accurate out to 1M which is orders of
magnitude greater than our largest possible GSO size.

This would then be implemented as:
return (((size >> 12) * 85) >> 8) + ICE_DESCS_FOR_SKB_DATA_PTR;

Since multiplication and division are commutative, we can reorder
operations into:
return ((size * 85) >> 20) + ICE_DESCS_FOR_SKB_DATA_PTR;
