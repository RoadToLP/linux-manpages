.TH "genwqe_user_vmap" 9 "genwqe_user_vmap" "May 2021" "Kernel Hacker's Manual" LINUX
.SH NAME
genwqe_user_vmap \- Map user-space memory to virtual kernel memory
.SH SYNOPSIS
.B "int" genwqe_user_vmap
.BI "(struct genwqe_dev *cd "  ","
.BI "struct dma_mapping *m "  ","
.BI "void *uaddr "  ","
.BI "unsigned long size "  ");"
.SH ARGUMENTS
.IP "cd" 12
pointer to genwqe device
.IP "m" 12
mapping params
.IP "uaddr" 12
user virtual address
.IP "size" 12
size of memory to be mapped
.SH "DESCRIPTION"
We need to think about how we could speed this up. Of course it is
not a good idea to do this over and over again, like we are
currently doing it. Nevertheless, I am curious where on the path
the performance is spend. Most probably within the memory
allocation functions, but maybe also in the DMA mapping code.

Restrictions: The maximum size of the possible mapping currently depends
on the amount of memory we can get using \fBkzalloc\fP for the
page_list and pci_alloc_consistent for the sg_list.
The sg_list is currently itself not scattered, which could
be fixed with some effort. The page_list must be split into
PAGE_SIZE chunks too. All that will make the complicated
code more complicated.
.SH "RETURN"
0 if success
